# -*- coding: utf-8 -*-
"""Wine quality dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HqePGQ-H-HsD-0SJrbCgpLFFzbWGHGuo

#Neural Networks Model For This Dataset

We try combining both the datasets and see how far we can go with this.
"""

import numpy as np
import pandas as pd
import seaborn as sns
from imblearn.over_sampling import RandomOverSampler
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import copy
from keras.utils import to_categorical

cols = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
       'pH', 'sulphates', 'alcohol', 'quality']

red_wine = pd.read_csv('winequality-red.csv', names = cols, delimiter = ';')
white_wine = pd.read_csv('winequality-white.csv', names = cols, delimiter = ';' )

red_wine.head()
white_wine.head()


#You can use sep = '\s+' to seperate the white columns within the dataset if needed.
#You can also use something called delimiter = ';' since for this dataset the boundaries were set by ';' between each code.

red_wine['type'] = 'red'
white_wine['type'] = 'white'

wine_data = pd.concat([red_wine, white_wine], ignore_index = True)
wine_data.head()

#'ignore_index = True' basically ensures that the index of the resulting DataFrame is reset to a new continuous integer. :)

print(wine_data.info())
print(wine_data.describe())
print(wine_data.columns)

#This process is done to esure that the data has no missing values and checking which feature can be dropped.

sns.histplot(wine_data['quality'], kde = True )
plt.show()

sns.pairplot(wine_data,hue='type',vars = ['fixed acidity', 'volatile acidity','citric acid'] ,palette={'red' : 'r', 'white' : 'b'}, diag_kind ='hist' )
plt.show()

#I dont know wtf happened to the sns.pairplot but yeah.
#We have something

wine_data['type'] = wine_data['type'].map({'red' : 1, 'white' : 0})

for col in cols:
    try:
       wine_data[col] = pd.to_numeric(wine_data[col], errors = 'coerce')
    except ValueError as e:
      print(f'Conversion error in column {col} : {e}')


corr_matrix = wine_data.corr()
sns.heatmap(corr_matrix, annot = True, cmap='coolwarm')
plt.show()

scaler = StandardScaler()
X = wine_data.drop(['quality', 'type'], axis = 1)
y = wine_data['quality']
X_scaled = scaler.fit_transform(X)

y_class = pd.cut(y, bins = [0,5,6,10], labels = ['0', '1', '2'])

wine_data['type'] = wine_data['type'].map({'red' : 0 , 'white' : 1 })

for col in wine_data.columns:
  if wine_data[col].dtype == 'object':
    try:
      wine_data[col] = pd.to_numeric(wine_data[col], errors = 'coerce')
    except:
      print(f'Could not convert column {col} to numeric.')

train_wine_data, valid_wine_data, test_wine_data = np.split( wine_data.sample(frac = 1), [int(0.6*len(wine_data)), int(0.8*len(wine_data))])

def get_xy(dataframe, y_label, x_labels = None):
  dataframe = copy.deepcopy(dataframe)
  if x_labels[0] is None:
    X = dataframe[[c for c in dataframe.columns if c!=y_label]].value
  else:
    if len(x_labels) == 1:
       X = dataframe[x_labels[0]].values.reshape(-1,1)
    else:
      X = dataframe[x_labels].values

  y =  dataframe[y_label].values.reshape(-1,1)
  data = np.hstack((X,y))

  return data, X, y

data_train, X_train, y_train = get_xy(train_wine_data, 'quality', x_labels = wine_data.columns[1:])
data_valid, X_valid, y_valid = get_xy(valid_wine_data, 'quality', x_labels = wine_data.columns[1:])
data_test, X_test, y_test = get_xy(test_wine_data, 'quality', x_labels = wine_data.columns[1:])

valid_mask = y_train < 10  # Assuming valid wine quality labels are between 0 and 10
valid_mask = valid_mask.reshape(3899)
X_train_valid = X_train[valid_mask]
y_train_valid = y_train[valid_mask]

# Re-check for unique labels after filtering
unique_labels_valid = np.unique(y_train_valid)
print("Unique labels after filtering:", unique_labels_valid)

unique_labels = np.unique(y_train)
print("Unique labels:", unique_labels)

# Identify invalid labels
invalid_labels = unique_labels[unique_labels > 10]  # Assuming valid wine quality labels are between 0 and 10
if len(invalid_labels) > 0:
    print("Invalid labels found:", invalid_labels)
else:
    print("No invalid labels found.")

# Function to check for invalid labels
def check_invalid_labels(y_train):
    unique_labels = np.unique(y_train)
    print("Unique labels:", unique_labels)

    # Identify invalid labels (assuming valid wine quality labels are between 0 and 10)
    invalid_labels = unique_labels[unique_labels > 10]
    if len(invalid_labels) > 0:
        print("Invalid labels found:", invalid_labels)
    else:
        print("No invalid labels found.")

# Function to remove invalid labels
def remove_invalid_labels(X_train, y_train):
    valid_mask = y_train <= 10  # Assuming valid wine quality labels are between 0 and 10
    X_train_valid = X_train[valid_mask.flatten()]
    y_train_valid = y_train[valid_mask]

    return X_train_valid, y_train_valid

# Check for invalid labels in the training data
check_invalid_labels(y_train)

# Remove invalid labels
X_train_valid, y_train_valid = remove_invalid_labels(X_train, y_train)

# Re-check for unique labels after filtering
check_invalid_labels(y_train_valid)

y_test = to_categorical(y_test, num_classes = 0)

def train_model(X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs):
    unique_labels, y_train = np.unique(y_train, return_inverse=True)
    num_classes = len(unique_labels)

    print(f"Unique classes: {np.unique(y_train)}")
    print(f"Number of classes: {num_classes}")
    print(f"Shape of X_train: {X_train.shape}, dtype: {X_train.dtype}")
    print(f"Shape of y_train: {y_train.shape}, dtype: {y_train.dtype}")

    invalid_labels = y_train[(y_train < 0 ) | (y_train >= num_classes)]
    if len(invalid_labels) > 0:
       print(f'Invalid Labels found: {invalid_labels}')
       return None,None,None

    if num_classes == 2:
      activation = 'sigmoid'
      loss = 'binary_crossentropy'
      num_output_nodes = 1
    else:
      activation = 'softmax'
      loss = 'categorical_crossentropy'
      num_output_nodes = num_classes

    nn_model = tf.keras.Sequential([
      tf.keras.layers.Dense(64 , activation = 'relu', input_shape = (X_train.shape[1],)),
      tf.keras.layers.Dropout(dropout_prob),
      tf.keras.layers.Dense(32, activation = 'relu'),
      tf.keras.layers.Dropout(dropout_prob),
      tf.keras.layers.Dense(num_output_nodes, activation = activation)
])

    nn_model.compile(optimizer = tf.keras.optimizers.Adam(lr), loss = loss,
                 metrics = ['accuracy'])
    history = nn_model.fit(
      X_train, y_train, epochs = epochs , batch_size = batch_size, validation_split = 0.2, verbose = 0
  )
    return nn_model, history, unique_labels

#There is another problem faced here where the tf.keras.layers.Dense(1, activation = 'sigmoid').
#This is used for a binary classification.
#Whereas tf.keras.layers.Dense(num_classes, activation = 'softmax')
#This is used for a multi-class classification problem.
#We add the num_classes = len(np.unique(y_train)) because the y_train data for this dataset contains label values outside the range of 7
#7 because the valid range is [0,1)

def plot_history(history):
  fig, (ax1,ax2) = plt.subplots(1,2, figsize = (10,4))
  ax1.plot(history.history['loss'], label = 'loss')
  ax1.plot(history.history['val_loss'], label = 'val_loss')
  ax1.set_xlabel('Epoch')
  ax1.set_ylabel('Binary crossentropy')
  ax1.grid(True)

  ax2.plot(history.history['accuracy'], label = 'accuracy')
  ax2.plot(history.history ['val_accuracy'], label = 'val_accuracy')
  ax2.set_xlabel('Epochs')
  ax2.set_ylabel('Accuracy')
  ax2.grid(True)

  plt.show()

X_scaled = np.asarray(X_scaled).astype(np.float32)
X_train = np.asarray(X_train).astype(np.float32)
X_test = np.asarray(X_test).astype(np.float32)
y_train = np.asarray(y_train).astype(np.float32)
y_test = np.asarray(y_test).astype(np.float32)
y_class = np.asarray(y_class).astype(np.float32)
X_train = np.array(X_train, dtype = np.float32)
y_train = np.array(y_train, dtype = np.float32)
y_test = y_test.astype(np.int32)





#This conversion into float was done because when we return the {model, history} without this sequence a problem would be faced.
#That problem is "The NumPy array was failed to transform in to a Tensor"
#Simply due to the fact that when I encode the variable i left it as a string instead of "0,1,2"
#This cause the failure of conversion
#I will link a sample question, a fellow programmer asked in stack overflow below

"""https://stackoverflow.com/questions/64407964/failed-to-convert-a-numpy-array-to-a-tensor"""

least_val_loss = float('inf')
least_loss_model = None
epochs = 100
for num_nodes in [16, 32, 64]:
  for dropout_prob in [0, 0.2]:
    for lr in [0.01, 0.005, 0.001]:
      for batch_size in [32, 64, 128]:
        print(f'{num_nodes} nodes, dropout {dropout_prob}, lr{lr} , batch size {batch_size} ')
        model, history, _ = train_model(X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs)
        if model is None:
          print("Skipping due to invalid labels")
          continue
        plot_history(history)

        val_loss = model.evaluate(X_test, y_test, verbose = 0)
        if val_loss[0] < least_val_loss:
          least_val_loss = val_loss[0]
          least_loss_model = model


print(f"Best model for validation loss: {least_val_loss}")

"""This is the end of our project. Throughout the entire fiesco, I tried combining both the datasets of red and white wine to see where it would take me. Seeing that I am facing multiple repeated errors simply mean that the approach is not worth the effort. Not because giving up is easy but simply due to thorough research. Seeing that combining two seperate datasets will affect the quality of precision and accuracy is enough of a reason to scrap this model. This has been a stepping stone for me in Machine Learning and I am now more confident than ever to see how far I can go with this."""